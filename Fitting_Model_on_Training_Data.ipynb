{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    " \n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
